{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK/afHh4TjCrrBwGXck1FY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bythyag/generative-ai-projects/blob/main/gemma-2b-yoda-adapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref blog: https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face"
      ],
      "metadata": {
        "id": "4xj8-Mt8qGQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "br1Ou_GopmTq"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install -q transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "id": "v-u1PBgVp2v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "repo_id = 'google/gemma-2b-it'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "2QGWhUjoqknS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check space taken by the model\n",
        "print(model.get_memory_footprint()/1e6)"
      ],
      "metadata": {
        "id": "LUhlXeGfrWWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# improves numerical stability during training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "xD9ylHVrrhlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
        "    r=8,\n",
        "    lora_alpha=16, # multiplier, usually 2*r\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "aGEASJKnr516"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ],
      "metadata": {
        "id": "us0BlRoBsJQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_p, tot_p = model.get_nb_trainable_parameters()\n",
        "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
        "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
        "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')"
      ],
      "metadata": {
        "id": "89IMm7Y6s-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")"
      ],
      "metadata": {
        "id": "NHT6-q_Rs_d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "Ygc7NakPtEqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "dataset = dataset.remove_columns([\"translation\"])"
      ],
      "metadata": {
        "id": "_DFiutjgtNdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "G3KZakuOtXqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(examples):\n",
        "    if isinstance(examples[\"prompt\"], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[\"prompt\"])):\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "            ]\n",
        "            output_texts.append(converted_sample)\n",
        "        return {'messages': output_texts}\n",
        "    else:\n",
        "        converted_sample = [\n",
        "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "        ]\n",
        "        return {'messages': converted_sample}\n",
        "\n",
        "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])\n",
        "dataset[0]['messages']"
      ],
      "metadata": {
        "id": "NYMBzjeCtaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "tokenizer.chat_template"
      ],
      "metadata": {
        "id": "xppKlIvxtfih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = dataset[0]['messages']\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "id": "FE7l1ry7tshP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "X2LsZzT7uCtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = SFTConfig(\n",
        "    ## GROUP 1: Memory usage\n",
        "    # These arguments will squeeze the most out of your GPU's RAM\n",
        "    # Checkpointing\n",
        "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
        "    # Set this to avoid exceptions in newer versions of PyTorch\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    # Gradient Accumulation / Batch size\n",
        "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
        "    gradient_accumulation_steps=1,\n",
        "    # The initial (micro) batch size to start off with\n",
        "    per_device_train_batch_size=16,\n",
        "    # If batch size would cause OOM, halves its size until it works\n",
        "    auto_find_batch_size=True,\n",
        "\n",
        "    ## GROUP 2: Dataset-related\n",
        "    max_seq_length=64,\n",
        "    # Dataset\n",
        "    # packing a dataset means no padding is needed\n",
        "    packing=True,\n",
        "\n",
        "    ## GROUP 3: These are typical training parameters\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=3e-4,\n",
        "    # Optimizer\n",
        "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
        "    optim='paged_adamw_8bit',\n",
        "\n",
        "    ## GROUP 4: Logging parameters\n",
        "    logging_steps=10,\n",
        "    logging_dir='./logs',\n",
        "    output_dir='./gemma-2b-yoda-adapter',\n",
        "    report_to='none'\n",
        ")"
      ],
      "metadata": {
        "id": "t3p7cXKJuaCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fp16, left-padding can cause longer tokens to overflow the buffer because padding isn't masked efficiently.\n",
        "Right-padding ensures the active tokens are grouped at the front of the sequence, allowing better memory control\n",
        "and computational stability.\n",
        "'''\n",
        "tokenizer.padding_side = 'right'"
      ],
      "metadata": {
        "id": "g1H6KVf7vJ9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "ty_txUJxu68Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = trainer.get_train_dataloader()\n",
        "batch = next(iter(dl))"
      ],
      "metadata": {
        "id": "HUBfoCOeu7FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'][0], batch['labels'][0]"
      ],
      "metadata": {
        "id": "fzXsdUcGvYig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Imt9zT1bvbZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_prompt(tokenizer, sentence):\n",
        "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        converted_sample, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "5dZUDRYsvhBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Meet me at the other side of the hill!'\n",
        "prompt = gen_prompt(tokenizer, sentence)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "MV8kcVv8zPws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
        "    tokenized_input = tokenizer(\n",
        "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    gen_output = model.generate(**tokenized_input,\n",
        "                                eos_token_id=tokenizer.eos_token_id,\n",
        "                                max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
        "    return output[0]"
      ],
      "metadata": {
        "id": "PZEx4XqkzV-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, tokenizer, prompt))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6J-z6ji9zcv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('local-gemma-2b-yoda-adapter')"
      ],
      "metadata": {
        "id": "IZaKgbyEzfuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "EQLA2KEgzj9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "a19-it15z3jj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}